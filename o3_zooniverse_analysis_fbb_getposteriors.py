# -*- coding: utf-8 -*-
"""Zooniverse_analysis_FBB_02012026.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F31qiErgAra-4Bdd7sru_KMyb_H-IcMy
"""

###IMPORTANT!!!! CHECK FOR ######REMOVE!!!!!!! FOR SHORTCUTS THAT MUST BE REMOVED

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import seaborn as sns
from tqdm import tqdm
from IPython.display import Image, display, HTML
import warnings
import json
import os
import time
from zooniverse_utils import *
from zooniverse_config import *
from o1_zooniverse_analysis_fbb_processclassifications import processclassifications

epsilon = 1e-10


def calc_users_score(z, ournames, ground_truth):
    scoring = []
    print("\n\n\n##### calculating user scores")
    for uid in tqdm(z["user_id"].unique()): # for every user in a df of classified subjects that the core team classifies
      uname = z[z["user_id"] == uid]["user_name"].unique()
      if uname in ournames:
        #skip us
        continue

      zuid = z[z["user_id"] == uid] # this is the DF for that one user
      _ = zuid.merge(ground_truth, left_on="subjects_id", right_index=True) #merging with our scores to

      score = (_["label"] == _["ground_truth"]).sum() / _.shape[0]
      scoring.append((score, len(_), uid, uname))

    return scoring

def get_skills(a):
  
  def S(x):
    return x * np.log2(x)
  TP,  TN = a['TP'], a['TN']
  if TP == 1:
    TP = 1-epsilon
  if TP == 0:
    TP = epsilon
  if TN == 1:
    TN = 1-epsilon
  if TN == 0:
    TN = epsilon
  skill = p_R * (S(TP) + S(1-TP)) #0.5*(-1)
  skill = skill + (1-p_R) * (S(TN) + S(1-TN)) # 0.5 * (-1)  = -1
  skill = skill - S(p_R * TP + (1-p_R) * (1-TN)) #+ 0.5
  skill = skill - S(p_R * (1-TP) + (1-p_R) * TN) #+0.5
  return skill

def precompute_user_histories(zooniverse, ground_truth):
    """Precompute cumulative skill histories for all users - FIXED VERSION."""
    # Convert dates
    
    zooniverse = zooniverse.copy()
    zooniverse['created_at_dt'] = pd.to_datetime(zooniverse['created_at'])
    zooniverse = zooniverse.sort_values(['user_id', 'created_at_dt'])
    
    # Ground truth dict - ensure it's a dict
    if hasattr(ground_truth, 'to_dict'):
        gt_dict = ground_truth.to_dict()
    else:
        gt_dict = ground_truth

    # Group by user
    user_histories = {}
    
    for user_id, group in tqdm(zooniverse.groupby('user_id')):
        #print(group)
        # Convert dates to Unix timestamps for faster comparison
        dates = group['created_at_dt'].values.astype('datetime64[s]').astype('int64')
        subjects = group['subject_ids'].values
        labels = group['label'].values
        
        # Pre-allocate result arrays
        n = len(dates)
        tp_counts = np.zeros(n, dtype=int)
        tn_counts = np.zeros(n, dtype=int)
        real_counts = np.zeros(n, dtype=int)
        bogus_counts = np.zeros(n, dtype=int)
        
        # Calculate cumulative counts
        current_tp = 0
        current_tn = 0
        current_real = 0
        current_bogus = 0
        
        for i in range(n): # for every subject
            subject_id = subjects[i]
            label = labels[i]
            # the time series have the same value at the next step if the subject is not in the gt set
            if subject_id in gt_dict["ground_truth"]:
                gt = gt_dict["ground_truth"][subject_id]
                
                if gt == "Real":
                    current_real += 1
                    if label == "Real":
                        current_tp += 1
                else:  # "Bogus"
                    current_bogus += 1
                    if label == "Bogus":
                        current_tn += 1
            
            tp_counts[i] = current_tp #cumulative number of tp until now
            tn_counts[i] = current_tn #cumulative number of tn until now
            real_counts[i] = current_real #cumulative number of real labels issued until now
            bogus_counts[i] = current_bogus #cumulative number of bogus labels issued until now
        
        user_histories[user_id] = {
            'dates': dates,  #these are all time series long n
            'tp_counts': tp_counts,
            'tn_counts': tn_counts,
            'real_counts': real_counts,
            'bogus_counts': bogus_counts
        }
       
    return user_histories


def calc_user_skills(user_histories, currentdate, user, verbose=False):
    agent = {}
    
    if user not in user_histories:
        agent.update({"TP": 0.5, "TN": 0.5, "FP": 0.5, "FN": 0.5})
        return agent, get_skills(agent)
    
    history = user_histories[user]
    dates = history['dates']  # These are Unix timestamps (int64)

    # Convert currentdate to Unix timestamp
    currentdate_dt = pd.to_datetime(currentdate)
    currentdate_ts = currentdate_dt.timestamp()  # Convert to Unix timestamp
    
    # Find the last index where date < currentdate
    
    idx = np.searchsorted(dates, currentdate_ts, side='right') - 1 #Find indices where elements should be inserted to maintain order
    
    if idx >= 0:
        tp = history['tp_counts'][idx]
        tn = history['tn_counts'][idx]
        real = history['real_counts'][idx]
        bogus = history['bogus_counts'][idx]
        
        #print(tp, tn, real, bogus)
        agent["TP"] = tp / real if real > 0 else 0.5
        agent["TN"] = tn / bogus if bogus > 0 else 0.5
        agent["FP"] = 1 - agent["TN"] if bogus > 0 else 0.5
        agent["FN"] = 1 - agent["TP"] if real > 0 else 0.5
    else:
        agent.update({"TP": 0.5, "TN": 0.5, "FP": 0.5, "FN": 0.5})
        agent
    if verbose:
        print(f"TP {agent['TP']} TN {agent['TN']}")
    for a in agent:
      if agent[a] == 1: agent[a] -= epsilon
      if agent[a] == 0: agent[a] += epsilon

    return agent, get_skills(agent)

  
def get_subject_posterior(subject, classifications, zooniverse,
                                   full_ground_truth, user_histories):

    p = p_R
    
    skill = [0]
    preals = [p]
    labels = [0.5]
    
    for i in classifications.index: #loop over classifications

      label = classifications.loc[i, "label"]
      user = classifications.loc[i, "user_id"]
      currentdate = classifications.loc[i, "created_at"]
      
      agent, sk = calc_user_skills(user_histories, currentdate, user)

      with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        if label == "Real":
          p_real_given_real = agent["TP"] * p / (agent["TP"] * p + agent["FP"] * (1-p))
          #if p = 1 => tp/tp is 1 either way but if tp=0 this returns nan - skip
          #if p = 0 => is 0 either way but if fp=0 this returns nan - skip
        if label == "Bogus":
          p_real_given_real = agent["FN"] * p / (agent["FN"] * p + agent["TN"] * (1-p))

      p = p_real_given_real if not np.isnan(p_real_given_real) else p
      #print(p)

      preals.append(p)
      skill.append(sk)
      labels.append(1 if label=="Real" else 0)
      warnings.filterwarnings('default')

    return preals, skill, labels
 
  #load_parallel_dicts_dataframe(filein.replace("zooniverse", "posteriors").replace(".csv", ".pqt"))
  
def user_analysis(filein, date, read_ground_truth=False, verbose=False, plotit=False):
  zooniverse, multilabel, users = readinzooniverse(filein)
  #number of labels per user
  _ = zooniverse.groupby(["user_id", "label"]).count()

  # Reset the index so 'user_id' is a column, then merge with unique user mappings
  _ = _.reset_index().merge(zooniverse[["user_id", "user_name"]].drop_duplicates(),
                            on="user_id").set_index(["user_id", "label"])


  # Filter and remove the 'label' index level so only 'user_id' remains as the index
  nreal = _.query('label == "Real"')[["classification_id", "user_name_y"]].reset_index(level="label", drop=True)
  nbogus = _.query('label == "Bogus"')[["classification_id"]].reset_index(level="label", drop=True)

  # Rename columns to avoid collision during merge
  nreal = nreal.rename(columns={"classification_id": "nreal"})
  nbogus = nbogus.rename(columns={"classification_id": "nbogus"})

  # Now you can merge them on the index (user_id)
  user_stats = nreal.merge(nbogus, left_index=True, right_index=True, how="outer").fillna(0)
  user_stats["RBratio"] = user_stats["nreal"] / user_stats["nbogus"]
  with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    user_stats["logRBratio"] = np.log10(user_stats["RBratio"])
  user_stats["nall"] = user_stats["nreal"] + user_stats["nbogus"]
  user_stats["Rfraction"] = user_stats["nreal"] / user_stats["nall"]
  user_stats.sort_values(by="nall", inplace=True)

  if plotit:
    user_stats[np.isfinite(user_stats["RBratio"])].plot(
      x="nall", y="RBratio", kind="scatter", alpha=0.5)
    plt.xscale("log")
    plt.yscale("log")
    plt.title("what is the distribution of real to bogus ratio?")
    plt.ylabel("Real to Bogus label ratio")
    plt.xlabel("number of labels by user")  

    user_stats[np.isfinite(user_stats["Rfraction"])].hist("Rfraction", alpha=0.5)
    plt.title("what is the distribution of real to bogus ratio?")  
    plt.xlabel("fraction of Real labels")
    plt.show()
  


  user_stats["mean"] = user_stats["logRBratio"].rolling(
    window=100, min_periods=1, center=True).mean()
  user_stats["stdev"] = user_stats["logRBratio"].rolling(
    window=100, min_periods=1, center=True).std()

  user_stats["mean_p2stdev"] = user_stats["mean"] + user_stats["stdev"] * 2
  user_stats["mean_m2stdev"] = user_stats["mean"] - user_stats["stdev"] * 2

  if plotit:
    ax = user_stats[np.isfinite(user_stats["RBratio"])].plot(
      x="nall", y="logRBratio", kind="scatter", alpha=0.5)

    user_stats.plot(ax=ax, x="nall", y="mean", color="k")
    user_stats.sort_values(by=("nall")).plot(
      ax=ax, x="nall", y="mean_p2stdev", color="k", alpha=0.5)
    user_stats.sort_values(by=("nall")).plot(
      ax=ax, x="nall", y="mean_m2stdev", color="k", alpha=0.5)
    plt.xscale("log")
    plt.ylabel("Real to Bogus label ratio")
    plt.xlabel("number of labels by user ")    
    plt.show()
  
    ax = zooniverse[multilabel].groupby("user_name").count().iloc[:,:1].hist(label="user name", alpha=0.5)
    plt.yscale('log')
    zooniverse[multilabel].groupby("user_id").count().iloc[:,:1].hist(ax=ax, label="user id", alpha=0.5)
    plt.yscale('log')
    plt.title("number of user by name by number of classifications they performed\n"
              "(some users are anonymous: name = NaN)");
    plt.legend()
    plt.show()

  if verbose:
    print("description of the users dataframe")
    display(zooniverse[multilabel]["user_name"].describe())

  ##### create rb dataframe ######
  rbdf, multilabelsubjects = createRBdf(zooniverse, multilabel, verbose=verbose, readsaved=False)

  ############get ground truths############

  subjectsfile = 'rubin-difference-detectives-subjects_{date}.csv'
  full_ground_truth = pd.read_csv(f"outputs/full_ground_truth_{date}.csv", index_col=0)

  subjects_w_gt =  full_ground_truth.index.unique()
  if verbose:
    print(f"{subjects_w_gt.shape[0]} subjects with ground_truth")

  ##  scoring users by agreement w us
  gt = zooniverse[multilabel]["subjects_id"].isin(subjects_w_gt)

  if verbose:
    print("\n\n\n##### ground truth classifications with multiple labels")
    display(zooniverse[multilabel][gt])
  z = zooniverse[multilabel][gt].copy()
  userprofiles = pd.DataFrame(calc_users_score(z, ournames, full_ground_truth),
                              columns=["score", "nlabels", "user_id", "user_name"])
  del z
  
  userprofiles.set_index("user_id", inplace=True)
  userprofiles = userprofiles.merge(zooniverse.groupby("user_id").count().iloc[:,:1],
                  left_index=True, right_index=True)
  userprofiles.rename({"classification_id":"totaluserlabels"}, axis=1, inplace=True)

  if plotit:
    userprofiles.plot(x="totaluserlabels", y="score", kind="scatter", alpha=0.5)
    plt.xscale("log")
    plt.show()

  print(f"\n\n\n##### saving user profile file as outputs/zooniverse_userprofiles_{date}.csv")
  userprofiles.to_csv(f"outputs/zooniverse_userprofiles_{date}.csv")    
  
  assert get_skills({"TN":0.5, "TP":0.5}) == 0

  userprofiles["skillnow"] = 0

  #marshalletal_dictionary = {"nlabels":"experience", "totallabels":"effort",
  #                         "skill":"skill"}

  classifications_by_scored_users = zooniverse[[u in userprofiles.index.values
                                                for u in zooniverse["user_id"]]]
  classification_count = classifications_by_scored_users.groupby("subject_ids").count()["classification_id"]

  if verbose:
    print("number of subjects classified by users with user profile",
        classification_count.shape[0])

  subjects_with_multiple_scored_labels = classification_count[classification_count>4].index
  Nplots = 25 
  fig, axs = plt.subplots(int(Nplots / 5), 5)
  Nplots  = int(Nplots / 5) * 5
  
  axs = axs.ravel()
    
  print("\n\n\n##### calculating subject posteriors")

  print("\n\n\n##### getting user histories")
  start = time.perf_counter()
  user_histories = precompute_user_histories(zooniverse, full_ground_truth)
  print("#### saving histories to file")
  save_user_histories_parquet(user_histories,
                              filein.replace("zooniverse", "user_histories").replace(".csv", ".pqt"))
  #user_histories = load_user_histories_parquet(filein.replace("zooniverse", "user_histories" ))

  preals = {}
  skill = {}
  labels = {}

  print(f"\n\n\n##### Getting posteriors for {len(subjects_with_multiple_scored_labels)} subjects")
  for subject in tqdm(subjects_with_multiple_scored_labels):    
          
    select = classifications_by_scored_users["subjects_id"] == subject
    _ = classifications_by_scored_users[select].sort_values("created_at")
    preals[subject], skill[subject], labels[subject] = get_subject_posterior(subject, _,
                                   zooniverse, full_ground_truth, user_histories)
  end = time.perf_counter()
  print(f"Execution time get_subject_posterior: {end - start:.6f} seconds")  

  for s in range(Nplots):
    subject = subjects_with_multiple_scored_labels[s]
    axs[s].plot(preals[subject], 'k-', label="posterior(R)")
    #axs[s].plot(ic[subject], 'r.', label="skill proxy")
    axs[s].plot(labels[subject], 'r.', label="label", alpha=0.5)
    axs[s].plot(skill[subject], 'b-', label="IC skill", alpha=0.5)
    axs[s].set_xlim(-0.1,17)
    axs[s].set_ylim(-0.05,1.05)
    axs[s].set_title(f"{subject}", fontsize=10)

    axs[s].axis('off')

  axs[-2].legend(loc='best')
  plt.show()

  save_parallel_dicts_to_dataframe(preals, skill, labels,
                                   subjects_with_multiple_scored_labels,
                                   filein.replace("zooniverse", "posteriors").replace(".csv", ".pqt"))

  subject = 113935471
  posterior = {"preals":preals, "skills":skill, "labels":labels}
  show_triplet_and_posteriors(subject, zooniverse, posteriors=posterior)



if __name__ == "__main__":
  filein = f'outputs/zooniversedf_saved_{date}.csv'
  if TEST:
    filein = filein.replace(".csv", "_short.csv")
  user_analysis(filein, date, verbose=False, plotit=False)
